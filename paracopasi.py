import os
from os.path import dirname
import time
import subprocess
import pandas as pd
from mpi4py.futures import MPIPoolExecutor
from utility.path_manipulate import expandabspath as eap
from utility.path_manipulate import makedirs_with_check as safemkdirs
from utility.path_manipulate import rel2abspath
from cpsmanager import cpsmanager
# task_type = ['tcs', 'pe', 'ps', 'opt', 'NA']


class paracopasi:
    _DEFAULT_COPASI_PATH = 'CopasiSE'
    _DEFAULT_WORKSPACE = '~/paracopasi_workspace/'
    _DEFAULT_RESULT_DIR = 'results/'
    _DEFAULT_MAX_WORKERS = 4

    class _task_spec:
        def __init__(self,
                     id=None,
                     cpsfilepath=None,
                     num_of_repeats=1,
                     type='pe'):
            self.id = id
            self.cpsfilepath = cpsfilepath
            self.num_of_repeats = num_of_repeats
            self.type = type

    def __init__(self,
                 copasise=None,
                 workspace=_DEFAULT_WORKSPACE,
                 result_dir=_DEFAULT_RESULT_DIR,
                 max_workers=_DEFAULT_MAX_WORKERS,
                 task_batch=None,
                 task_log=None,
                 speedup_summary=None):
        if copasise is None:
            self.copasise = subprocess.check_output(
                ['which', self._DEFAULT_COPASI_PATH]).strip().decode('utf-8')
        else:
            self.copasise = eap(copasise)
        self.config_dir = '{}/share/copasi/config/'.format(
            dirname(dirname(self.copasise)))
        self.workspace = eap(workspace)
        self.result_dir = eap(result_dir)
        self.max_workers = max_workers
        if task_batch is None:
            self.tasks = None
        else:
            self.task_batch = task_batch
            self.tasks = []
        self.task_stats = {'different_tasks': 0}
        if task_log is None:
            self.task_log = os.path.join(
                self.workspace, '{}_workers.log'.format(self.max_workers))
        else:
            self.task_log = task_log
        if speedup_summary is None:
            self.speedup_summary = os.path.join(
                self.workspace, '{}_workers.stats'.format(self.max_workers))

    def add_task(self, id=None, cpsfilepath=None, num_of_repeats=1, type='pe'):
        task = self._task_spec(id, cpsfilepath, num_of_repeats, type)
        self.tasks.append(task)

    def load(self, source=None):
        df = source
        if source is None:
            local_tbpath = self.task_batch
        elif type(source) is str:
            local_tbpath = source
        if os.path.exists(local_tbpath):
            df = pd.read_csv(local_tbpath)
        else:
            raise FileNotFoundError('Task specification batch file not found.')
        for row in df.itertuples(index=False):
            self.add_task(id=row.taskID,
                          cpsfilepath=rel2abspath(
                              os.path.dirname(self.task_batch),
                              row.model_file),
                          num_of_repeats=row.number_of_repeats,
                          type='NA')

    def _get_task_list(self, df=None):
        """
        input
        ----------------------------------------------------------------
        df: pandas DataFrame or str. Generated by reading the file specifying tasks.
        str indicates the file path. 
        
        return
        ----------------------------------------------------------------
        A tuple of the following lists.
        taskID: list of int. ID of the task specified in the batch file.
        cpsfilepath: list of str.
        order_of_repeat: list of int. Order of repeat of the task.
        type_of_task: list of str.
        """
        taskID = []
        cpsfilepath = []
        order_of_repeat = []
        type_of_task = []
        for t in self.tasks:
            taskID.extend([t.id] * t.num_of_repeats)
            cpsfilepath.extend([t.cpsfilepath] * t.num_of_repeats)
            type_of_task.extend(['NA'] * t.num_of_repeats)
            if t.id not in self.task_stats:
                self.task_stats['different_tasks'] += 1
                self.task_stats[t.id] = 0
            order_of_repeat.extend(
                range(self.task_stats[t.id],
                      self.task_stats[t.id] + t.num_of_repeats))
            self.task_stats[t.id] += t.num_of_repeats
        return (taskID, cpsfilepath, order_of_repeat, type_of_task)

    def run_cps(self, id, cpsfilepath, type, order_of_repeat):
        return paracopasi.run_cps_to_result_dir(id, cpsfilepath, type,
                                                order_of_repeat,
                                                self.result_dir)

    @staticmethod
    def run_cps_to_result_dir(id,
                              cpsfilepath,
                              type,
                              order_of_repeat,
                              result_dir,
                              copasise='CopasiSE'):
        _, cpsfilename = os.path.split(cpsfilepath)
        output_path = os.path.join(
            result_dir, '{}{}.{}'.format(cpsfilename, order_of_repeat, type))
        cmd = [copasise, cpsfilepath, '--report-file', output_path, '--nologo']
        start_time = time.perf_counter()
        subprocess.run(cmd)
        end_time = time.perf_counter()
        time_spent = end_time - start_time
        return id, order_of_repeat, start_time, end_time, time_spent

    def save_task_log(self, result_set):
        res_df = pd.DataFrame(result_set,
                              columns=[
                                  'taskID', 'order_of_repeat',
                                  'cpu_time_start', 'cpu_time_end',
                                  'cpu_time_spent'
                              ])
        res_df.to_csv(self.task_log, index=False)
        return res_df
    
    def save_task_log_pe(self, result_set):
        res_df = pd.DataFrame(result_set,
                              columns=[
                                  'taskID', 'order_of_repeat',
                                  'cpu_time_start', 'cpu_time_end',
                                  'cpu_time_spent', 'cps_edit_time',
                                  'cps_save_remove_time'
                              ])
        res_df.to_csv(self.task_log, index=False)
        return res_df

    def save_speedup_summary(self, task_log_df, wall_clock_time_start,
                             wall_clock_time_end):
        total_cpu_time = sum(task_log_df['cpu_time_spent'])
        wall_clock_time = wall_clock_time_end - wall_clock_time_start
        speedup = total_cpu_time / wall_clock_time
        efficiency = speedup / self.max_workers
        d = {
            'wall_clock_time_start': wall_clock_time_start,
            'wall_clock_time_end': wall_clock_time_end,
            'wall_clock_time': wall_clock_time,
            'total_cpu_time': total_cpu_time,
            'speedup': speedup,
            'efficiency': efficiency
        }
        summary_df = pd.DataFrame(data=d, index=[0])
        summary_df.to_csv(self.speedup_summary, index=False)
        return summary_df
    
    def save_speedup_summary_df(self, task_log_df, wall_clock_time_start,
                             wall_clock_time_end):
        total_cpu_time = sum(task_log_df['cpu_time_spent'])
        total_cps_edit_time = sum(task_log_df['cps_edit_time'])
        total_cps_save_remove_time = sum(task_log_df['cps_save_remove_time'])
        wall_clock_time = wall_clock_time_end - wall_clock_time_start
        speedup = total_cpu_time / wall_clock_time
        efficiency = speedup / self.max_workers
        d = {
            'wall_clock_time_start': wall_clock_time_start,
            'wall_clock_time_end': wall_clock_time_end,
            'wall_clock_time': wall_clock_time,
            'total_cpu_time': total_cpu_time,
            'speedup': speedup,
            'efficiency': efficiency,
            'total_cps_edit_time': total_cps_edit_time,
            'total_cps_save_remove_time': total_cps_save_remove_time
        }
        summary_df = pd.DataFrame(data=d, index=[0])
        summary_df.to_csv(self.speedup_summary, index=False)
        return summary_df

    def launch(self):
        safemkdirs(self.workspace)
        safemkdirs(self.result_dir)
        task_and_order = self._get_task_list()
        wall_clock_time_start = time.perf_counter()
        with MPIPoolExecutor(max_workers=self.max_workers) as executor:
            result_set = executor.map(self.run_cps,
                                      task_and_order[0],
                                      task_and_order[1],
                                      task_and_order[2],
                                      task_and_order[3],
                                      unordered=True,
                                      chunksize=1)
        wall_clock_time_end = time.perf_counter()
        task_log_df = self.save_task_log(result_set)
        self.save_speedup_summary(task_log_df, wall_clock_time_start,
                                  wall_clock_time_end)

    def multiple_pe(self, cpsfile, parameter_set, taskID='pe0'):
        """
        parameter_set:
        [
            [
                (name1,lower1,upper1,initial1),
                (name2,lower2,upper2,initial2),
                (name3,lower3,upper3,initial3),
            ......
            ]
            ......
        ]
        """
        cpsfile = eap(cpsfile)
        if not os.path.isfile(cpsfile):
            raise FileNotFoundError('.cps file not found.')
        temp_dir = os.path.join(self.workspace, 'temp')
        safemkdirs(temp_dir)
        num = len(parameter_set)
        safemkdirs(self.result_dir)
        wall_clock_time_start = time.perf_counter()
        with MPIPoolExecutor(max_workers=self.max_workers) as executor:
            result_set = executor.map(paracopasi.single_pe, [cpsfile] * num,
                                      parameter_set,
                                      range(len(parameter_set)),
                                      [self.result_dir] * num,
                                      [taskID] * num, [temp_dir] * num,
                                      unordered=True,
                                      chunksize=1)
        wall_clock_time_end = time.perf_counter()
        task_log_df = self.save_task_log_pe(result_set)
        self.save_speedup_summary_df(task_log_df, wall_clock_time_start,
                                  wall_clock_time_end)
        os.rmdir(temp_dir)
        return None

    @staticmethod
    def single_pe(cpsfile,
                  parameter_set,
                  order,
                  result_dir,
                  taskID='pe0',
                  temp_dir='temp/'):
        cm = cpsmanager(cpsfile)
        cpsfile_dir, filename = os.path.split(cpsfile)
        cps_edit_start = time.perf_counter()
        cm.parse()
        [
            cm.set_pe_parameter(*parameter_set[i])
            for i in range(len(parameter_set))
        ]
        cm.ensure_experiment_abspath(cpsfile_dir)
        cps_edit_end = time.perf_counter()
        cpsfilepath = os.path.join(
            temp_dir, '{}_{}_{}.cps'.format(filename, taskID, order))
        cm.save(cpsfilepath)
        cps_save_end = time.perf_counter()
        res = paracopasi.run_cps_to_result_dir(taskID, cpsfilepath, 'pe',
                                                order, result_dir)
        os.remove(cpsfilepath)
        return res + (cps_edit_end - cps_edit_start, cps_save_end - cps_edit_end)


if __name__ == '__main__':
    # The following is deprecated testbench
    # ------------------------------------------------------
    # copasise = '~/COPASI-4.29.228-Linux-64bit/bin/CopasiSE'
    # workspace = '~/paracopasi_workspace/'
    # result_dir = 'results'
    # pc = paracopasi(copasise=copasise,
    #                 workspace=workspace,
    #                 result_dir=result_dir,
    #                 max_workers=1)
    # # generate task list
    # task_list_path = 'testbench/task_list.csv'
    # pc.load(task_list_path)
    # pc.launch()

    # MPI command wrapper for paracopasi_cmd.py
    from paracopasi_cmd import get_parser
    parser = get_parser()
    args = parser.parse_args()
    if not os.path.exists(args.task_batch):
        raise FileNotFoundError('Task specification batch file not found.')
    pc = paracopasi(args.copasise, args.workspace, args.result_dir,
                    args.max_workers, args.task_batch, args.task_log,
                    args.speedup_summary)
    pc.load()
    pc.launch()
